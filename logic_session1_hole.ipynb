{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "logic_session1_hole.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RudigerLei/oTree/blob/master/logic_session1_hole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEyHhHF39Bui",
        "colab_type": "text"
      },
      "source": [
        "## Usage\n",
        "Like a regular notebook:\n",
        "- __run a cell__ with **_ctrl-enter_** or **_shift-enter_**\n",
        "- __use the command palette__ with **_ctrl-shift-P_** to find more complex commands\n",
        "\n",
        "Use it only with __Chrome__\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oy8-Z4EV2nQm",
        "colab_type": "code",
        "outputId": "7ed2e272-64f8-44de-e610-8c890271be31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PowerTransformer, MinMaxScaler\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwCsLx1L47iW",
        "colab_type": "text"
      },
      "source": [
        "# Session 1\n",
        "\n",
        "This introduction will altern theory & practice, and will be divided as follow:\n",
        "1. Configuration\n",
        "2. Text pre-processing\n",
        "3. Text embedding:\n",
        "    - Count Vectorizer\n",
        "    - TF-IDF\n",
        "    - N-grams\n",
        "4. Modelling:\n",
        "    - Logistic Regression\n",
        "    - Neural Network\n",
        "5. Text representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z175aef45DLp",
        "colab_type": "text"
      },
      "source": [
        "## 0. Data exploration\n",
        "\n",
        "Let's download datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NtCfm105Eas",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def download_data():\n",
        "    import urllib.request\n",
        "    import os\n",
        "    folder = './data'\n",
        "    list_urls = [\n",
        "        ('https://bendrive.s3-eu-west-1.amazonaws.com/test_set.csv', 'test_set.csv'),\n",
        "        ('https://bendrive.s3-eu-west-1.amazonaws.com/test_set_with_rating.csv', 'test_set_with_rating.csv'),\n",
        "        ('https://bendrive.s3-eu-west-1.amazonaws.com/train_set.csv', 'train_set.csv')\n",
        "        \n",
        "    ]\n",
        "    if not os.path.isdir(folder):\n",
        "        os.mkdir(folder)\n",
        "    for url, file in list_urls:\n",
        "        if not os.path.isfile(folder + '/' + file):\n",
        "            print(\"Downloading .. \", file)\n",
        "            urllib.request.urlretrieve(url, folder + '/' + file)\n",
        "\n",
        "download_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Wk1Gd9t5H8V",
        "colab_type": "text"
      },
      "source": [
        "With pandas, load the datasets into dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qz3-y46M5LkD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "df_test_w_rating = pd.read_csv(\"./data/test_set_with_rating.csv\")\n",
        "df_test = pd.read_csv(\"./data/test_set.csv\")\n",
        "df_train = pd.read_csv(\"./data/train_set.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6z_JlWRH5Nt8",
        "colab_type": "text"
      },
      "source": [
        "Load only a part of the train data for convenience"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-fSwdjO5ONU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = df_train[0:30000]\n",
        "df_train.head(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZMr4tvT5QOM",
        "colab_type": "text"
      },
      "source": [
        "We will focus on review and rating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrb2docl5Sk6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.set_option('display.max_colwidth', -1)\n",
        "\n",
        "df_train = df_train[['review', 'rating']].dropna()\n",
        "df_train['rating'] = df_train['rating'].astype('int')\n",
        "\n",
        "# Show a good review\n",
        "print(\"GOOD REVIEW\")\n",
        "display(df_train[df_train['rating']==5].sample(1))\n",
        "\n",
        "# Show a bad review\n",
        "print(\"BAD REVIEW\")\n",
        "display(df_train[df_train['rating']==1].sample(1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nmUESo65VGT",
        "colab_type": "text"
      },
      "source": [
        "Let's look at the number of comments for each rating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdYKTl0Q5XWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(df_train['rating'].value_counts())\n",
        "print(df_train['rating'].value_counts(normalize=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1juKwcl95ZRC",
        "colab_type": "text"
      },
      "source": [
        "Let's look at the distribution in a more convenient way"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4trsy2zW5bRM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "df_train[\"rating\"].hist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5xN2YHS5dG3",
        "colab_type": "text"
      },
      "source": [
        "# 1. Problematic\n",
        "\n",
        "Are we able to predict the rating from the text comment ?\n",
        "\n",
        "So our goal is to have a model, a function, that takes text as input and output a rating.\n",
        "\n",
        "$f(text) = rating$\n",
        "\n",
        "Are **you** able to do it ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZr9fic35ftY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Show a bad review\n",
        "import numpy as np\n",
        "\n",
        "print(\"GUESS THE RATING ?\")\n",
        "random_rating = np.random.randint(1, 6)\n",
        "display(df_train[df_train['rating']==random_rating]['review'].sample(1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bkCgPfb5l5O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(random_rating)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hBW7xg75n7h",
        "colab_type": "text"
      },
      "source": [
        "To begin with a binary classification problem, we will bin the target into bad review and good review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4adKDkHc5ocv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train['bin_rating'] = df_train['rating'].apply(# CODE HERE) # warning: hole 1\n",
        "\n",
        "df_train[\"bin_rating\"].hist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uwz2ON-5qkS",
        "colab_type": "text"
      },
      "source": [
        "# 2. From text to numbers\n",
        "\n",
        "First step is to transform text into numbers, to go from:\n",
        "\n",
        "    f(text) = rating\n",
        "\n",
        "to\n",
        "\n",
        "    f(numerical_vector_representing_text) = rating\n",
        "\n",
        "For example:\n",
        "\n",
        "    \"Meh.  Nothing special.  Food was just ok.\"\n",
        "\n",
        "must be transformed to something like:\n",
        "\n",
        "    [0, 4, 2, 8]\n",
        "\n",
        "where the values of the vector convey meaning of the text. Two different texts must have a different representation:\n",
        "\n",
        "    \"Meh.  Nothing special.  Food was just ok.\" > [0, 4, 2, 8]\n",
        "    \n",
        "    \"Aww.  Spectacular.  Food was amaaaaazing.\" > [4, 2, 6, 1]\n",
        "\n",
        "Also, it would be more convenient if texts of different sizes can translate to a fixed size vector, for example:\n",
        "\n",
        "    \"Meh.  Nothing special.  Food was just ok.\" > [0, 4, 2, 8]\n",
        "    \n",
        "    \"Meh.\" > [0, 3, 3, 7]\n",
        "\n",
        "\n",
        "## 2.1 From one text to a list of tokens\n",
        "\n",
        "First step is to convert a text, which is one string of characters, to a list of words.\n",
        "\n",
        "    \"Meh.  Nothing special.\" > [\"meh\", \"nothing\", \"special\"]\n",
        "    \n",
        "Then, those words must be transformed into tokens, wich are representation of words. This can be done with\n",
        "\n",
        "(1) **Stemming** is the process of eliminating affixes (suffixed, prefixes, infixes, circumfixes) from a word in order to obtain a word stem.\n",
        "\n",
        "(2) **Lemmatization** is related to stemming, differing in that lemmatization is able to capture canonical forms based on a word's lemma.\n",
        "\n",
        "(3) **Everything else**: set all characters to **lowercase**, remove numbers (or convert numbers to textual representations), remove **punctuation** (generally part of tokenization, but still worth keeping in mind at this stage, even as confirmation), strip white space (also generally part of tokenization), remove default  **stop words**  (general English stop words), etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vz2KQGP-5uZR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_text_to_lowercase(df, colname):\n",
        "    df[colname] = df[colname].str.lower()\n",
        "    return df\n",
        "    \n",
        "def not_regex(pattern):\n",
        "        return r\"((?!{}).)\".format(pattern)\n",
        "\n",
        "def remove_punctuation(df, colname):\n",
        "    df[colname] = df[colname].str.replace('\\n', ' ')\n",
        "    df[colname] = df[colname].str.replace('\\r', ' ')\n",
        "    alphanumeric_characters_extended = '(\\\\b[-/]\\\\b|[a-zA-Z0-9])'\n",
        "    df[colname] = df[colname].str.replace(not_regex(alphanumeric_characters_extended), ' ')\n",
        "    return df\n",
        "\n",
        "def tokenize_sentence(df, colname):\n",
        "    df[colname] = df[colname].str.split()\n",
        "    return df\n",
        "\n",
        "def remove_stop_words(df, colname):\n",
        "    stop_words = stopwords.words('english')\n",
        "    df[colname] = df[colname].apply(lambda x: [word for word in x if word not in stop_words])\n",
        "    return df\n",
        "\n",
        "def reverse_tokenize_sentence(df, colname):\n",
        "    df[colname] = df[colname].map(lambda word: ' '.join(word))\n",
        "    return df\n",
        "\n",
        "\n",
        "def text_cleaning(df, colname):\n",
        "    \"\"\"\n",
        "    Takes in a string of text, then performs the following:\n",
        "    1. convert text to lowercase\n",
        "    2. remove punctuation and new line characters '\\n'\n",
        "    3. Tokenize sentences\n",
        "    4. Remove all stopwords\n",
        "    5. convert tokenized text to text\n",
        "    \"\"\"\n",
        "    df = (\n",
        "        df\n",
        "        .pipe(convert_text_to_lowercase, colname)\n",
        "        .pipe(remove_punctuation, colname)\n",
        "        .pipe(tokenize_sentence, colname)\n",
        "        .pipe(remove_stop_words, colname)\n",
        "        .pipe(reverse_tokenize_sentence, colname)\n",
        "    )\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ykt6PWd5zdS",
        "colab_type": "text"
      },
      "source": [
        "- clean the review column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sNUjm0y533I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_cleaned = text_cleaning(df_train, 'review')\n",
        "\n",
        "df_cleaned.sample(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeNjzzuL56Av",
        "colab_type": "text"
      },
      "source": [
        "## 2.2. From a list of tokens to an embedding (numerical vector)\n",
        "\n",
        "We still have to transform the list of tokens to a fixed size numerical vector\n",
        "\n",
        "    [\"meh\", \"nothing\", \"special\"] > [0, 2, 8, 1]\n",
        "    [\"my\", \"belly\", \"is\", \"happy\"] > [2, 1, 9, 3]\n",
        "    [\"i\", \"took\", \"a\", \"loan\", \"to\", \"eat\", \"there\"] > [1, 1, 3, 8]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nP5wq2p6APd",
        "colab_type": "text"
      },
      "source": [
        "For that, we will use a very common technique called TF-IDF\n",
        "\n",
        "**Definition**\n",
        "\n",
        "> **TF-IDF** or **term frequency–inverse document frequency**, is a numerical statistic that is intended to reflect how important a word is to reflect how important a word is to a document in a collection or corpus.\n",
        "\n",
        "**TF: Term Frequency**, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization:  \n",
        "      \n",
        "    TF(t) = (Nbr of times term t appears in a document) / (Total nbr of terms in the document)\n",
        "    \n",
        "**IDF: Inverse Document Frequency**, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following:  \n",
        "      \n",
        "    IDF(t) = log_e(Total number of documents / Number of documents with term t in it)\n",
        "\n",
        "_Example:_\n",
        "_Consider a document containing 100 words wherein the word _cat_ appears 3 times. The term frequency (i.e., tf) for cat is then (3 / 100) = 0.03. Now, assume we have 10 million documents and the word _cat_ appears in one thousand of these. Then, the inverse document frequency (i.e., idf) is calculated as log(10,000,000 / 1,000) = 4. Thus, the Tf-idf weight is the product of these quantities: 0.03 * 4 = 0.12._\n",
        "\n",
        "**Tools**\n",
        "\n",
        "Python **sklearn** package **feature_extraction**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpdWvc3U6A57",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "number_of_dimensions = # CODE HERE\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    analyzer=,\n",
        "    ngram_range=,\n",
        "    max_features=,\n",
        "    max_df=,\n",
        "    min_df=) # CODE HERE\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(df_cleaned[\"review\"]).toarray()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dXZ4_vG6DlZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "randrow = np.random.randint(100)\n",
        "\n",
        "print(\" == Text\\n\")\n",
        "print(df_cleaned[\"review\"][randrow])\n",
        "print(\"\\n == Rating\\n\")\n",
        "print(df_cleaned[\"rating\"][randrow])\n",
        "print(\"\\n == Embedding\\n\")\n",
        "print(X_tfidf[randrow])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrmO42vr6Hkm",
        "colab_type": "text"
      },
      "source": [
        "## 3. Modelling\n",
        "\n",
        "Yeah ! We now have a numerical representation of the text, and the rating associated. Remember we start with a binary classification task: bad review / good review."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_-Fb-wl6IHT",
        "colab_type": "text"
      },
      "source": [
        "#### Split data into train / test sets\n",
        "\n",
        "In order to measure the performance of our modeling, we need to split the data into:\n",
        "\n",
        "- The **training set** contains a known output and the model learns on this data in order to be generalized to other data later on (= 80%)\n",
        "\n",
        "- We have the **test set** in order to test our model’s prediction on this test set (= 20%)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjp-bX_66MUM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TARGET = 'bin_rating'\n",
        "FEATURE = 'review'\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    # CODE HERE, \n",
        "    # CODE HERE, \n",
        "    test_size=# CODE HERE,\n",
        "    random_state=# CODE HERE) # warning: hole 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YH1WLD96M_p",
        "colab_type": "text"
      },
      "source": [
        "## 3.1 Baseline model\n",
        "\n",
        "As a baseline model, let's use a random model with a probability of good/bad review based on the distribution of the train set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ztwy8Cu56PTq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred_baseline = np.random.choice(\n",
        "    [0, 1],\n",
        "    size=len(y_test),\n",
        "    p=[1-y_train.mean(), y_train.mean()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaIsN2hS6Rim",
        "colab_type": "text"
      },
      "source": [
        "We need now to evaluate our baseline model.\n",
        "\n",
        "- **True Positives** (TP): these are the correctly predicted positive values which means that the value of actual class is yes and the value of predicted class is also yes. *E.g. if actual class value indicates that this passenger survived and predicted class tells you the same thing.*\n",
        "\n",
        "- **True Negatives** (TN): these are the correctly predicted negative values which means that the value of actual class is no and value of predicted class is also no. *E.g. if actual class says this passenger did not survive and predicted class tells you the same thing.*\n",
        "\n",
        "*False positives and false negatives, these values occur when your actual class contradicts with the predicted class.*\n",
        "\n",
        "- **False Positives** (FP): when actual class is no and predicted class is yes. *E.g. if actual class says this passenger did not survive but predicted class tells you that this passenger will survive.*\n",
        "\n",
        "- **False Negatives** (FN): when actual class is yes but predicted class in no. *E.g. if actual class value indicates that this passenger survived and predicted class tells you that passenger will die.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x20PqpFj6UIK",
        "colab_type": "text"
      },
      "source": [
        "- **Accuracy**: Accuracy is the most intuitive performance measure and it is simply a ratio of correctly predicted observation to the total observations. One may think that, if we have high accuracy then our model is best. Yes, accuracy is a great measure but only when you have symmetric datasets where values of false positive and false negatives are almost same. Therefore, you have to look at other parameters to evaluate the performance of your model.\n",
        "\n",
        "> Accuracy = TP+TN/TP+FP+FN+TN\n",
        "\n",
        "*= 154656 + 68551 / 154656 + 14186 + 9438 +68551 = 0.90*\n",
        "\n",
        "- **Precision**: Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. The question that this metric answer is of all passengers that labeled as survived, how many actually survived? High precision relates to the low false positive rate.\n",
        "\n",
        "> Precision = TP/TP+FP\n",
        "\n",
        "*= 154656 / 154656 + 14186 = 0.92*\n",
        "\n",
        "- **Recall (Sensitivity)**: Recall is the ratio of correctly predicted positive observations to the all observations in actual class - yes. The question recall answers is: Of all the passengers that truly survived, how many did we label?\n",
        "\n",
        "> Recall = TP/TP+FN\n",
        "\n",
        "*= 154656 / 154656 + 9438 = 0.94*\n",
        "\n",
        "- **F1 score**: F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. Intuitively it is not as easy to understand as accuracy, but F1 is usually more useful than accuracy, especially if you have an uneven class distribution. Accuracy works best if false positives and false negatives have similar cost. If the cost of false positives and false negatives are very different, it’s better to look at both Precision and Recall.\n",
        "\n",
        "> F1 Score = 2*(Recall * Precision) / (Recall + Precision)\n",
        "\n",
        "*= 2 * (0.94 * 0.92) / (0.94 + 0.92) = 0.93*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuObK8aL6ZT2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "confusion_matrix(y_test, y_pred_baseline, labels=None, sample_weight=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hte1zGa16bei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('accuracy {}'.format(accuracy_score(y_pred_baseline, y_test)))\n",
        "print(classification_report(y_test, y_pred_baseline))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8sS4caP6b9-",
        "colab_type": "text"
      },
      "source": [
        "## 3.2 Logistic Regression\n",
        "\n",
        "We can do better than a random model with logistic regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sDaRgm46efH",
        "colab_type": "text"
      },
      "source": [
        "#### Fit model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oi7IBc56glf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logit = LogisticRegression(solver='lbfgs', verbose=2, n_jobs=-1)\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('vectorizer', tfidf_vectorizer),\n",
        "    ('model', logit)])\n",
        "\n",
        "pipeline.fit(x_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWam65Tj6j8V",
        "colab_type": "text"
      },
      "source": [
        "#### Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtZD2-Sz6nBR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = pipeline.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6z-nzDm46pSF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "confusion_matrix(y_test, y_pred, labels=None, sample_weight=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gZwCJTY6rHk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('accuracy {}'.format(accuracy_score(y_pred, y_test)))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IT4RiNew6tQe",
        "colab_type": "text"
      },
      "source": [
        "Fortunately, logistic regression can do better than a random model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5onek-f6v52",
        "colab_type": "text"
      },
      "source": [
        "# 4. Neural Network\n",
        "\n",
        "## 4.1. Architecture\n",
        "\n",
        "Now, let's try to design a neural network model for our task.\n",
        "\n",
        "* What would be the input layer size ?\n",
        "* What would be the output layer size ?\n",
        "* What would be the output layer activation function ?\n",
        "  * Either relu or sigmoid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbBU79JE6zAH",
        "colab_type": "text"
      },
      "source": [
        "We store the architecture of the neural network in a list of dictionnaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdDy61Y46zz0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CODE BELOW\n",
        "# \n",
        "\n",
        "NN_ARCHITECTURE_0 = [\n",
        "    {\"input_dim\": ??, \"output_dim\": 8, \"activation\": \"relu or sigmoid ??\"},\n",
        "    {\"input_dim\": 8, \"output_dim\": 4, \"activation\": \"relu or sigmoid ??\"},\n",
        "    {\"input_dim\": 4, \"output_dim\": ??, \"activation\": \"relu or sigmoid ??\"},\n",
        "]\n",
        "\n",
        "NN_ARCHITECTURE_1 = [\n",
        "    {\"input_dim\": ??, \"output_dim\": 32, \"activation\": \"relu or sigmoid ??\"},\n",
        "    {\"input_dim\": 32, \"output_dim\": 16, \"activation\": \"relu or sigmoid ??\"},\n",
        "    {\"input_dim\": 16, \"output_dim\": 8, \"activation\": \"relu or sigmoid ??\"},\n",
        "    {\"input_dim\": 8, \"output_dim\": 4, \"activation\": \"relu or sigmoid ??\"},\n",
        "    {\"input_dim\": 4, \"output_dim\": ??, \"activation\": \"relu or sigmoid ??\"},\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCikVxpz64mp",
        "colab_type": "text"
      },
      "source": [
        "## 4.2. Parameters and initialization\n",
        "\n",
        "Each neuron has two parameters: weight (W) and bias (b).\n",
        "\n",
        "Let's initialize those parameters for each neuron and store them in a dictionnary with notation:\n",
        "* Wi is the array of the weights of the neurons of layer i\n",
        "* bi is the array of the biases of the neurons of the layer i"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVaFsP5-65LN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_layers(nn_architecture, seed = 99):\n",
        "    # random seed initiation\n",
        "    np.random.seed(seed)\n",
        "    # number of layers in our neural network\n",
        "    number_of_layers = len(nn_architecture)\n",
        "    # parameters storage initiation\n",
        "    params_values = {}\n",
        "    \n",
        "    # iteration over network layers\n",
        "    for idx, layer in enumerate(nn_architecture):\n",
        "        # we number network layers from 1\n",
        "        layer_idx = idx + 1\n",
        "        \n",
        "        # extracting the number of units in layers\n",
        "        layer_input_size = layer[\"input_dim\"]\n",
        "        layer_output_size = layer[\"output_dim\"]\n",
        "        \n",
        "        # initiating the values of the W matrix\n",
        "        # and vector b for subsequent layers\n",
        "        params_values['W' + str(layer_idx)] = #CODE HERE, should be an array of shape (layer_output_size, layer_input_size) \n",
        "        params_values['b' + str(layer_idx)] = #CODE HERE, should be an array of shape (layer_output_size, 1) \n",
        "        \n",
        "    return params_values\n",
        "\n",
        "params_values = init_layers(NN_ARCHITECTURE_0)\n",
        "\n",
        "for key, value in params_values.items():\n",
        "    print(key, value.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hb61uTDG6-36",
        "colab_type": "text"
      },
      "source": [
        "## 4.3. Activation functions\n",
        "\n",
        "What does a neuron do ?\n",
        "\n",
        "First it computes a value: Z, the linear combination of its inputs A by its weighted coefficients W and add a bias b. Then it applies an activation function to Z, and generates an output A.\n",
        "\n",
        "$$\\boldsymbol{Z}^{[l]} = \\boldsymbol{W}^{[l]} \\cdot \\boldsymbol{A}^{[l-1]} + \\boldsymbol{b}^{[l]}$$\n",
        "\n",
        "$$\\boldsymbol{A}^{[l]} = g^{[l]}(\\boldsymbol{Z}^{[l]})$$\n",
        "\n",
        "So, imagine a network where input layer has 3 neurons (layer 0), and next layer (layer 1) has 2 neurons.\n",
        "\n",
        "    N00\n",
        "            N10  \n",
        "    N01\n",
        "            N11\n",
        "    N02\n",
        "\n",
        "N10 would compute a value, like this:\n",
        "\n",
        "$$Z^{[10]} = W^{[10-00]} * A^{[00]} + W^{[10-01]} * A^{[01]} + W^{[10-02]} * A^{[02]} + b^{[10]}$$\n",
        "$$A^{[10]} = g^{[l]}(Z^{[10]})$$\n",
        "\n",
        "* What is the role of the activation function ?\n",
        "* Code the activation functions sigmoid and relu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxXqsEPz6_cH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(Z):\n",
        "    return # CODE HERE\n",
        "\n",
        "def relu(Z):\n",
        "    return # CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fz4mDBX-riWi",
        "colab_type": "code",
        "outputId": "303a81d1-76e8-4571-8a76-3192039e3fdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "sigmoid(-10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.5397868702434395e-05"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5prlOA8qfJT",
        "colab_type": "code",
        "outputId": "97509ed1-e488-41cb-f733-ba4a2f9ec8fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        }
      },
      "source": [
        "def test_sigmoid():\n",
        "  assert sigmoid(0)==0.5\n",
        "  assert sigmoid(10) > 0.9\n",
        "  assert sigmoid(-10) < 0.1\n",
        "\n",
        "def test_relu():\n",
        "  assert relu(-5) == 0\n",
        "  assert relu(5) == 5\n",
        "\n",
        "test_sigmoid()\n",
        "test_relu()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-4947412a8b7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtest_sigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtest_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-4947412a8b7b>\u001b[0m in \u001b[0;36mtest_relu\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0;32massert\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0;32massert\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsnSMwpV7BXj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.arange(-10, 20)\n",
        "plt.plot(x, sigmoid(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWDxwaif7Eb7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.arange(-10, 20)\n",
        "plt.plot(x, relu(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zddwzgzt7G00",
        "colab_type": "text"
      },
      "source": [
        "## 4.3. Forward propagation\n",
        "\n",
        "Forward propagation of a neural network is the left to right pass. It is the inference of the neural network model: from the input layer, the output layer is computed by forward propagation the value from one neuron to another.\n",
        "\n",
        "With the stored weights and biases of the neutwork, let's write the forward propagation functions.\n",
        "\n",
        "Reminder:\n",
        "\n",
        "\n",
        "$$\\boldsymbol{Z}^{[l]} = \\boldsymbol{W}^{[l]} \\cdot \\boldsymbol{A}^{[l-1]} + \\boldsymbol{b}^{[l]}$$\n",
        "\n",
        "$$\\boldsymbol{A}^{[l]} = g^{[l]}(\\boldsymbol{Z}^{[l]})$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "st8uP0mQ7Hf6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def single_layer_forward_propagation(A_prev, W_curr, b_curr, activation=\"relu\"):\n",
        "    # calculation of the input value for the activation function\n",
        "    Z_curr = # CODE HERE (numpy dot product function can be used)\n",
        "    \n",
        "    # selection of activation function\n",
        "    if activation is \"relu\":\n",
        "        activation_func = relu\n",
        "    elif activation is \"sigmoid\":\n",
        "        activation_func = sigmoid\n",
        "    else:\n",
        "        raise Exception('Non-supported activation function')\n",
        "        \n",
        "    # return of calculated activation A and the intermediate Z matrix\n",
        "    A_curr = # CODE HERE\n",
        "    return A_curr, Z_curr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h04NdH5e7J9a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def full_forward_propagation(X, params_values, nn_architecture):\n",
        "    # creating a temporary memory to store the information needed for a backward step\n",
        "    memory = {}\n",
        "    # X vector is the activation for layer 0 \n",
        "    A_curr = X\n",
        "    \n",
        "    # iteration over network layers\n",
        "    for idx, layer in enumerate(nn_architecture):\n",
        "        # we number network layers from 1\n",
        "        layer_idx = idx + 1\n",
        "        # transfer the activation from the previous iteration\n",
        "        A_prev = A_curr\n",
        "        \n",
        "        # extraction of the activation function for the current layer\n",
        "        activ_function_curr = layer[\"activation\"]\n",
        "        # extraction of W for the current layer\n",
        "        W_curr = params_values[\"W\" + str(layer_idx)]\n",
        "        # extraction of b for the current layer\n",
        "        b_curr = params_values[\"b\" + str(layer_idx)]\n",
        "        # calculation of activation for the current layer\n",
        "        A_curr, Z_curr = single_layer_forward_propagation(A_prev, W_curr, b_curr, activ_function_curr)\n",
        "        \n",
        "        # saving calculated values in the memory\n",
        "        memory[\"A\" + str(idx)] = A_prev\n",
        "        memory[\"Z\" + str(layer_idx)] = Z_curr\n",
        "       \n",
        "    # return of prediction vector and a dictionary containing intermediate values\n",
        "    return A_curr, memory"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLym6eI07MHU",
        "colab_type": "text"
      },
      "source": [
        "Let's try with a random observation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Aow_EMf7OWB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random_input = np.random.randn(number_of_dimensions, 1)\n",
        "output, _ = full_forward_propagation(random_input, params_values, NN_ARCHITECTURE_0)\n",
        "print(\"Output value is:\", output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsXVh9IW7SSs",
        "colab_type": "text"
      },
      "source": [
        "Let's now try with our train data embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LnJ75k_7S2V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_features = tfidf_vectorizer.fit_transform(x_train).toarray()\n",
        "x_train_features.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dvdilA17U0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output, _ = full_forward_propagation(np.transpose(x_train_features), params_values, NN_ARCHITECTURE_0)\n",
        "output.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0D1D6JkW7X-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ts3KMcdA7aBT",
        "colab_type": "text"
      },
      "source": [
        "So, we are able to infer a neural net to our data ! But are we good ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muPcrRXk7ca7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(output[0:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DI2JgTdv7c6v",
        "colab_type": "text"
      },
      "source": [
        "The output is now a proba, we need to convert it to a class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcUtAAFU7e2q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_prob_into_class(probs):\n",
        "    probs_ = np.copy(probs)\n",
        "    probs_[probs_ > 0.5] = 1\n",
        "    probs_[probs_ <= 0.5] = 0\n",
        "    return probs_\n",
        "\n",
        "def get_accuracy_value(Y_hat, Y):\n",
        "    Y_hat_ = convert_prob_into_class(Y_hat)\n",
        "    return (Y_hat_ == Y).all(axis=0).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOIDsztH7ibJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred_first_inference = convert_prob_into_class(output)\n",
        "print(y_pred_first_inference[0:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tl8juFkG7k7u",
        "colab_type": "text"
      },
      "source": [
        "So how does it look ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVJMUTll7m6k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('accuracy {}'.format(accuracy_score(np.squeeze(y_pred_first_inference), y_train)))\n",
        "print(classification_report(np.squeeze(y_pred_first_inference), y_train))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKks8-Y-7pTV",
        "colab_type": "text"
      },
      "source": [
        "BAD !\n",
        "\n",
        "Wait a minute, we forgot to train the neural network !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5U7fY9BU7poz",
        "colab_type": "text"
      },
      "source": [
        "## 4.4 Measuring progress\n",
        "\n",
        "Evaluating with a classification report is nice for the human to understand the model performances. But in order to train the model, we need a simpler and computational measure off the model fitness.\n",
        "\n",
        "This measure is the **loss**. The loss can be different from the metric that we - as human - use to evaluate the model.\n",
        "\n",
        "We should routinely calculate the value of the loss function. \"Generally speaking, the loss function is designed to show how far we are from the 'ideal' solution.\" It is selected according to the problem we plan to solve. For binary classification, binary crossentropy is common."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsB-0QMD7r5-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_cost_value(Y_hat, Y):\n",
        "    # number of examples\n",
        "    m = Y_hat.shape[1]\n",
        "    # calculation of the cost according to the formula\n",
        "    cost = # warning: hole 6\n",
        "    return np.squeeze(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Kqm-T0x7vS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_cost_value(np.array([[0.3, 0.1, 0.3]]), np.array([[0.1, 0.1, 0.4]]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gl8U5XY7xC0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_cost_value(output, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbrreV7A70cr",
        "colab_type": "text"
      },
      "source": [
        "## 4.5 Computing gradients with backpropagation\n",
        "\n",
        "Our neural network is dumb, we have been able to compute a loss. Now we need to understand where this loss is coming from, do we have dumber neurons than others ? For that we need to compute the gradient of the loss for each parameter.\n",
        "\n",
        "For that, we start from the loss at the output layer. We compute the derivatives of each neuron in order to update the weights and we go on until we reach the input layer.\n",
        "\n",
        "This process is called backpropagation.\n",
        "\n",
        "In NN, we calculate the gradient of the loss function in respect to parameters, but backpropagation can be used to calculate derivatives of any function. The essence of this algorithm is the recursive use of a chain rule known from differential calculus - calculate a derivative of functions created by assembling other functions, whose derivatives we already know. This process - for one network layer - is described by the following formulas. Looking at the formulas, it becomes obvious why we decided to remember the values of the A and Z matrices for intermediate layers in a forward step.\n",
        "\n",
        "$$\\boldsymbol{dW}^{[l]} = \\frac{\\partial L }{\\partial \\boldsymbol{W}^{[l]}} = \\frac{1}{m} \\boldsymbol{dZ}^{[l]} \\boldsymbol{A}^{[l-1] T}$$\n",
        "\n",
        "$$\\boldsymbol{db}^{[l]} = \\frac{\\partial L }{\\partial \\boldsymbol{b}^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} \\boldsymbol{dZ}^{[l](i)}$$\n",
        "\n",
        "$$\\boldsymbol{dA}^{[l-1]} = \\frac{\\partial L }{\\partial \\boldsymbol{A}^{[l-1]}} = \\boldsymbol{W}^{[l] T} \\boldsymbol{dZ}^{[l]}$$\n",
        "\n",
        "$$\\boldsymbol{dZ}^{[l]} = \\boldsymbol{dA}^{[l]} * g'(\\boldsymbol{Z}^{[l]})$$\n",
        "\n",
        "\n",
        "So first, we need to compute the derivatives of the activation functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbWxvcfq71Do",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid_backward(dA, Z):\n",
        "    sig = sigmoid(Z)\n",
        "    return # warning: hole 7\n",
        "\n",
        "def relu_backward(dA, Z):\n",
        "    dZ = np.array(dA, copy = True)\n",
        "    dZ[Z <= 0] = 0;\n",
        "    return dZ"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itocFH7i73Tq",
        "colab_type": "text"
      },
      "source": [
        "Let's backpropagate now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiPkz34875Fq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def single_layer_backward_propagation(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=\"relu\"):\n",
        "    # number of examples\n",
        "    m = A_prev.shape[1]\n",
        "    \n",
        "    # selection of activation function\n",
        "    if activation is \"relu\":\n",
        "        backward_activation_func = relu_backward\n",
        "    elif activation is \"sigmoid\":\n",
        "        backward_activation_func = sigmoid_backward\n",
        "    else:\n",
        "        raise Exception('Non-supported activation function')\n",
        "    \n",
        "    # calculation of the activation function derivative\n",
        "    dZ_curr = backward_activation_func(dA_curr, Z_curr)\n",
        "    \n",
        "    # derivative of the matrix W\n",
        "    dW_curr = np.dot(dZ_curr, A_prev.T) / m\n",
        "    # derivative of the vector b\n",
        "    db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m\n",
        "    # derivative of the matrix A_prev\n",
        "    dA_prev = np.dot(W_curr.T, dZ_curr)\n",
        "\n",
        "    return dA_prev, dW_curr, db_curr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTLsop0977l_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def full_backward_propagation(Y_hat, Y, memory, params_values, nn_architecture):\n",
        "    grads_values = {}\n",
        "    \n",
        "    # number of examples\n",
        "    m = Y.shape[1]\n",
        "    # a hack ensuring the same shape of the prediction vector and labels vector\n",
        "    Y = Y.reshape(Y_hat.shape)\n",
        "    \n",
        "    # initiation of gradient descent algorithm\n",
        "    dA_prev = - (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat));\n",
        "    \n",
        "    for layer_idx_prev, layer in reversed(list(enumerate(nn_architecture))):\n",
        "        # we number network layers from 1\n",
        "        layer_idx_curr = layer_idx_prev + 1\n",
        "        # extraction of the activation function for the current layer\n",
        "        activ_function_curr = layer[\"activation\"]\n",
        "        \n",
        "        dA_curr = dA_prev\n",
        "        \n",
        "        A_prev = memory[\"A\" + str(layer_idx_prev)]\n",
        "        Z_curr = memory[\"Z\" + str(layer_idx_curr)]\n",
        "        \n",
        "        W_curr = params_values[\"W\" + str(layer_idx_curr)]\n",
        "        b_curr = params_values[\"b\" + str(layer_idx_curr)]\n",
        "        \n",
        "        dA_prev, dW_curr, db_curr = single_layer_backward_propagation(\n",
        "            dA_curr, W_curr, b_curr, Z_curr, A_prev, activ_function_curr)\n",
        "        \n",
        "        grads_values[\"dW\" + str(layer_idx_curr)] = dW_curr\n",
        "        grads_values[\"db\" + str(layer_idx_curr)] = db_curr\n",
        "    \n",
        "    return grads_values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yS-OfG3a7-_l",
        "colab_type": "text"
      },
      "source": [
        "## 4.6 Updating parameters - Teaching\n",
        "\n",
        "Now that we know the gradient of the loss according to all neurons, we can teach the model, by updating the weights according to their gradients and a given learning rate.\n",
        "\n",
        "This update parameter is simply done here with gradient descent. More complex optimizers exist."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRXYTVB28BSB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update(params_values, grads_values, nn_architecture, learning_rate):\n",
        "\n",
        "    # iteration over network layers\n",
        "    for layer_idx, layer in enumerate(nn_architecture, 1):\n",
        "        params_values[\"W\" + str(layer_idx)] = # CODE HERE   \n",
        "        params_values[\"b\" + str(layer_idx)] = # CODE HERE\n",
        "\n",
        "    return params_values;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "im_4MsKQ8Dj1",
        "colab_type": "text"
      },
      "source": [
        "## 4.7 Assembling bricks - Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7Ib6amO8GG2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(X, Y, nn_architecture, epochs=1000, learning_rate=0.1, verbose=False, callback=None):\n",
        "    # initiation of neural net parameters\n",
        "    params_values = init_layers(nn_architecture, 2)\n",
        "    # initiation of lists storing the history \n",
        "    # of metrics calculated during the learning process \n",
        "    cost_history = []\n",
        "    accuracy_history = []\n",
        "    \n",
        "    # performing calculations for subsequent iterations\n",
        "    for i in range(epochs):\n",
        "        # step forward\n",
        "        Y_hat, cashe = full_forward_propagation(X, params_values, nn_architecture)\n",
        "        \n",
        "        # calculating metrics and saving them in history\n",
        "        cost = get_cost_value(Y_hat, Y)\n",
        "        cost_history.append(cost)\n",
        "        accuracy = get_accuracy_value(Y_hat, Y)\n",
        "        accuracy_history.append(accuracy)\n",
        "        \n",
        "        # step backward - calculating gradient\n",
        "        grads_values = full_backward_propagation(Y_hat, Y, cashe, params_values, nn_architecture)\n",
        "        # updating model state\n",
        "        params_values = update(params_values, grads_values, nn_architecture, learning_rate)\n",
        "        \n",
        "        if(i % 200 == 0):\n",
        "            if(verbose):\n",
        "                print(\"Iteration: {:05} - cost: {:.5f} - accuracy: {:.5f}\".format(i, cost, accuracy))\n",
        "            if(callback is not None):\n",
        "                callback(i, params_values)\n",
        "            \n",
        "    return params_values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOGPzdAo8LtR",
        "colab_type": "text"
      },
      "source": [
        "## 4.8 Let's try"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7fmDcHV8X1D",
        "colab_type": "text"
      },
      "source": [
        "####  Archi 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsH9TwYa8M1O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training\n",
        "\n",
        "x_train_features = tfidf_vectorizer.fit_transform(x_train).toarray()\n",
        "\n",
        "y_train_array = np.array(y_train)\n",
        "\n",
        "params_values = train(np.transpose(x_train_features),\n",
        "                      np.transpose(y_train_array.reshape((y_train_array.shape[0], 1))),\n",
        "                      NN_ARCHITECTURE_0, epochs=10000, learning_rate=0.1,\n",
        "                      verbose=True\n",
        "                     )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beaItph48Vfp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test_features = tfidf_vectorizer.fit_transform(x_test).toarray()\n",
        "\n",
        "y_test_array = np.array(y_test)\n",
        "\n",
        "output, _ = full_forward_propagation(np.transpose(x_test_features),\n",
        "                                     params_values,\n",
        "                                     NN_ARCHITECTURE_0)\n",
        "y_pred_architecture_0 = convert_prob_into_class(output)\n",
        "\n",
        "print('accuracy {}'.format(accuracy_score(np.squeeze(y_pred_architecture_0), y_test)))\n",
        "print(classification_report(np.squeeze(y_pred_architecture_0), y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7lF4GFo8ZYO",
        "colab_type": "text"
      },
      "source": [
        "####  Archi 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLEj5-ML8Z7v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training\n",
        "\n",
        "x_train_features = tfidf_vectorizer.fit_transform( x_train).toarray()\n",
        "\n",
        "y_train_array = np.array(y_train)\n",
        "\n",
        "params_values_1 = train(np.transpose(x_train_features),\n",
        "                      np.transpose(y_train_array.reshape((y_train_array.shape[0], 1))),\n",
        "                      NN_ARCHITECTURE_1, epochs=10000, learning_rate=0.1,\n",
        "                      verbose=True\n",
        "                     )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_kxbswt8b4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test_features = tfidf_vectorizer.fit_transform(x_test).toarray()\n",
        "\n",
        "y_test_array = np.array(y_test)\n",
        "\n",
        "output, _ = full_forward_propagation(np.transpose(x_test_features),\n",
        "                                     params_values_1,\n",
        "                                     NN_ARCHITECTURE_1)\n",
        "y_pred_architecture_1 = convert_prob_into_class(output)\n",
        "\n",
        "print('accuracy {}'.format(accuracy_score(np.squeeze(y_pred_architecture_1), y_test)))\n",
        "print(classification_report(np.squeeze(y_pred_architecture_1), y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}